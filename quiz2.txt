Question 1:

1)
This statement is false. To actually fit a logistic regression, you have to first transform each element in the Y matrix with
this equation: ln(max/y - 1) (where y is the element)

2)
True, this is what the logistic prediction uses. It can't be negative since e^x can't be negative, and it can only go up to M at
max because it is m times 1/1+x, which can only go up to 1.

3)
This is true, as the inverse of the prediction function 1/(1+e^-x) is ln(1/x - 1)

4)
This is false. The prediction variable has to be transformed using this equation instead: ln(M/x - 1)
(Notice the place of M)

5)
True. Both logistic and linear regressions require matrix transposition, inverses, and multiplication, which will get you up to at the very
least O(n^3). With logistic regressions, the operations required to transform the y matrix and the prediction take a O(n), which we don't
consider because it is a couple orders of magnitude smaller than the rest of the operations.

6)
False. This is because the linear regression gets the *closest* line to the set of data. There is a possibility that two points will be
impossible to include in the line, so it will just get as close as possible.

7)
True. A directed graph is a graph where every node has both parents and children instead of just neighbors.
The difference between a tree and a directed graph is that the tree can't have any loops.

8)
False. Even with the highly inefficient method we built in class for pathfinding, we stop the algorithm right when the desired node is found.
This means if the desired node is on the first level it searches, it stops the algorithm and disregards any other nodes in the graph.

9)
False. You can do the same algorithm, except you just have to replace the nodes that get
searched on each nodes from the neighbors to the children.

10)
True. For proof, refer to problem 38-3-A

11)
True.
1 = sum(0) for -inf to a + sum(k) for a to b + sum(0) for b to inf
  = 0 + k(b-a) + 0
k = 1/(b-a)

12)
True. The probability function can be modeled as some c * L, where L is the likelihood and c is a constant.
In 11, we proved that c is equal to 1, so p(x) = L

=======================================
Question 2:
1)
L(k|flips) = P(1)P(2)P(3)P(4)
           = (1-P(4))^3/3 * P(4)
           = k(1-k)^3/3

1 = Int(c k(1-k)^3/3) from 0 -> 1
3 = c Int(k(1-k)^3) from 0 -> 1
  = c (1-k)^4/4 - (1-k)^5/5 from 0 -> 1
  = c/20
c = 60

P(x) = { 60k(1-k)^3/3 for 0 <= x <= 1
       { 0            otherwise

2)
L(k|flips) = P(4)P(4)P(4)
           = kkk = k^3

3)
1 = Int(c* k^3) from 0 -> 1
1/c = k^4/4 from 0 -> 1
1/c = 1/4
c = 4
p(x) = c * L

p_k(x) = { 4k^3 for 0 <= x <= 1
         { 0    otherwise

L(k|{4,4,4}) = k^3

4)
P(k > x) = 25%
(since it should be 1/4 times you get a 4)

1/4 = 1 - Int(4k^3) from 0 -> x
3/4 = k^4 from 0 -> x
3/4 = x^4
x = qdrt(3/4) ~= 93%



BONUS
1/6000000 = 1 - Int(4k^3) from 0 -> x
599999/6000000 = k^4 from 0 -> x
               = x^4

x = qdrt(599999/6000000)
x = 0.562

SOMETHING HAPPENED WITH PROBLEM 2
